import torch as th
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class AttentionQ(nn.Module):
    def __init__(self, args, hidden_dim = 32, norm_in = True, attention_heads=4):  
        '''
        args has
        args.n_agents
        args.n_actions
        args.state_shape

        overall explanation:

        key and value are generated by (obs + action encodings)
        query is generated by obs encodings

        after weight generation, # First layer in QMIX is replaced with weight and agent_qs multiplcation
        weight generation of attention should be in dimension of [batch * max_seq -1, n_agents, embed_dim]

        in order to extend space with state informations we use overall state input in #Second layer

        we should also generate optimizer and gradient scaler for AttentionQ 
        check https://github.com/shariqiqbal2810/MAAC/blob/6174a01251251e6778c4ada26bc8d9cd930e3856/algorithms/attention_sac.py#L110 

        '''
        super(AttentionQ, self).__init__()

        self.args = args
        self.n_agents = args.n_agents
        self.state_dim = int(np.prod(args.state_shape))
        self.action_dim = args.n_actions
        self.attention_heads = attention_heads
        self.hidden_dim = hidden_dim
        self.obs_dim = 80 # should be in args, but no time to fix!
        self.hypernet_embed = 64

        self.oa_encoders = nn.ModuleList()  # this is for key, value
        self.obs_encoders = nn.ModuleList() # this is for query

        for _ in range(self.n_agents):
            # generating oa_encoders for each agent (key, value)
            input_dim = self.obs_dim + self.action_dim
            oa_encoder = nn.Sequential()
            if norm_in:
                oa_encoder.add_module('oa_bn', nn.BatchNorm1d(input_dim,affine=False))
            oa_encoder.add_module('oa_fc1', nn.Linear(input_dim,self.hidden_dim))
            oa_encoder.add_module('oa_relu',nn.LeakyReLU())
            self.oa_encoders.append(oa_encoder)
            
            # generating obs_encoders for each agent (query)
            obs_encoder = nn.Sequential()
            if norm_in:
                obs_encoder.add_module('obs_bn',nn.BatchNorm1d(self.obs_dim,affine=False))
            obs_encoder.add_module('obs_fc1',nn.Linear(self.obs_dim,self.hidden_dim))
            obs_encoder.add_module('obs_relu',nn.LeakyReLU())
            self.obs_encoders.append(obs_encoder)
        
        atten_dim = self.hidden_dim // self.attention_heads
        # list of (key, value, query) generators for each attention heads
        self.key_generators = nn.ModuleList()
        self.query_generators = nn.ModuleList()
        self.value_generators = nn.ModuleList()

        for _ in range(self.attention_heads):
            self.key_generators.append(nn.Linear(self.hidden_dim,atten_dim, bias=False))
            self.query_generators.append(nn.Linear(self.hidden_dim,atten_dim, bias=False))
            self.value_generators.append(nn.Sequential(nn.Linear(self.hidden_dim,atten_dim),nn.LeakyReLU()))

        # this is for 2nd stage of 2-stage layers
        self.hyper_w_final = nn.Sequential(nn.Linear(self.state_dim, self.hypernet_embed), nn.ReLU(), nn.Linear(self.hypernet_embed, self.hidden_dim))
        self.V = nn.Sequential(nn.Linear(self.state_dim, self.hidden_dim), nn.ReLU(), nn.Linear(self.hidden_dim, 1))


    def forward(self,agent_qs, states, actions, obs):
        '''
        agent_qs : [batch_size, max_seq -1, n_agents]
        states : [batch_size, max_seq-1, state_dim]
        actions: [batch_size, max_seq-1, n_agents, 1] -> actions[0][0] contains [[],[],[],[],[]] integer between 1~11 // or 0~10 (?)
        if we were to use observation: [batch_size, max_seq-1, n_agent, obs_dimension]
        '''
        bs = agent_qs.size(0)
        agent_qs = agent_qs.view(-1, 1, self.n_agents) # [batch_size * max_seq-1, 1, n_agents]
        states = states.reshape(-1, self.state_dim) # [batch_size * max_seq-1, state_dim]
        seq_len = agent_qs.size(1)
        # transform action into one-hot encoding
        actions = actions.view(bs,seq_len,-1) # turn last two dim [n_agents,1] into [n_agents]
        actions = F.one_hot(actions, num_classes=self.action_dim) # actions : [batch_size, max_seq-1, n_agents, action_dim]
        
        # create observation + state encoding
        # obs = obs.view(-1,self.n_agents,self.obs_dim)  use reshape instead
        obs = obs.reshape(-1,self.n_agents,self.obs_dim)    # [batch_size * max_seq-1, n_agent, obs_dimension]
        obs = obs.permute(0,2,1) # [batch_size * max_seq-1, obs_dimension , n_agent]
        actions = actions.view(-1,self.n_agents,self.action_dim) # [batch_size * max_seq-1, n_agent, action_dimension]  
        actions = actions.permute(0,2,1) # [batch_size * max_seq-1, action_dimension , n_agent]
        oa_concat = th.cat((obs,actions),dim=1) # [batch_size * max_seq-1, obs_dimension + action_dimension , n_agent]
        oa_lists = list(oa_concat.split(1,dim=-1)) # make it into a python list to iterate -> this produces each [batch_size * max_seq-1, obs_dimension + action_dimension, 1]
        oa_encodings = [encoder(oa_list.squeeze()) for encoder, oa_list in zip(self.oa_encoders,oa_lists)]    # [batch_size * max_seq-1, hidden_dim , n_agent] but last dimension in list form
        # squeeze is added to delete additional dimension generated from above

        # create observation encoding
        obs_lists = list(obs.split(1,dim=-1)) # make it into a python list to iterate 
        obs_encodings = [encoder(obs_list.squeeze()) for encoder, obs_list in zip(self.obs_encoders,obs_lists)]

        # generate key, value using oa_encodings
        multihead_keys = [[key_gen(enc) for enc in oa_encodings] for key_gen in self.key_generators] # [batch_size * max_seq-1, attention_dim , n_agent, attention heads] but last two dimensions in list form
        mutlihead_values = [[value_gen(enc) for enc in oa_encodings] for value_gen in self.value_generators]
        # generate query using obs_encodings
        mutlihead_queries = [[query_gen(enc) for enc in obs_encodings] for query_gen in self.query_generators]

        weights_each_agent = [[] for _ in range(self.n_agents)]

        for singlehead_keys, singlehead_values, singlehead_queries in zip(multihead_keys, mutlihead_values, mutlihead_queries):
            # singlehead_queries : [batch_size * max_seq-1, attention_dim , n_agent] // but in list form

            for i, query in zip(range(self.n_agents), singlehead_queries):  # query : [batch_size * max_seq-1, attention_dim]
                keys = [key for j, key in enumerate(singlehead_keys) if j != i]
                values = [value for j, value in enumerate(singlehead_values) if j != i]

                # calculate attention across agents
                # key-query mult
                key_prod_query = th.matmul(query.view(query.shape[0],1,-1),th.stack(keys).permute(1,2,0)) # [batch_size * max_seq-1, 1, n_agents -1]
                '''
                query after view : [batch_size * max_seq-1, 1, attention_dim]
                after permute : [batch_size * max_seq-1, attention_dim, n_agents -1]
                '''
                # scale product with size of key
                scaled_key_prod_query = key_prod_query / np.sqrt(keys[0].shape[1]) # divide by root(attention_dim) : [batch_size * max_seq-1, 1, n_agents -1]
                scaled_key_prod_query = F.softmax(scaled_key_prod_query,dim=2) # softmax in agent dimension
                attention_weights = (th.stack(values).permute(1,2,0)* scaled_key_prod_query).sum(dim =2)    # [batch_size * max_seq-1, attention_dim]
                '''
                values after permute : [batch_size * max_seq-1, attention_dim, nagents-1]
                scaled_key_prod_query is broadcasted 
                after summation (dim=2) : [batch_size * max_seq-1, attention_dim]
                '''
                weights_each_agent[i].append(attention_weights) 

        # First layer - we are going to use weights_each_agent as weight 
        '''
        after 2-state for loop, [batch_size * max_seq-1, attention_dim, num_heads, n_agents] will be stored while last two dimensions are lists 
        '''
        # convert last two dimensions into tensor
        for i in range(self.n_agents):  # weight_each_agent[i] : [batch_size * max_seq-1, attention_dim, num_heads] list in last dimension
            weights_each_agent[i] = th.cat(weights_each_agent[i],dim=1) # [batch_size * max_seq-1, attention_dim * num_heads (hidden_dim)]
        weights_each_agent = th.stack(weights_each_agent,dim=-1)  # [batch_size * max_seq-1, hidden_dim, n_agents]

        w1 = th.tensor(weights_each_agent,dtype=th.float32 ,device=self.args.device) # am I allowed to do this? 
        w1 = w1.permute(0,2,1)     # [batch_size * max_seq-1, n_agents , hidden_dim]
        hidden = F.elu(th.bmm(agent_qs,w1)) # [batch_size * max_seq-1, 1, hidden_dim]
        
        # add state information in the second layer
        w_final = self.hyper_w_final(states)    # [batch_size * max_seq-1, hidden_dim]
        w_final = w_final.view(-1,self.hidden_dim,1)    # [batch_size * max_seq-1, hidden_dim, 1]
        # bias from state 
        v = self.V(states).view(-1,1,1) # [batch_size * max_seq-1, 1, 1]
        y = th.bmm(hidden,w_final) + v # [batch_size * max_seq-1, 1, 1]
        # Reshape and return
        q_tot = y.view(bs,-1,1) # [batch_size, max_seq-1, 1]
        return q_tot